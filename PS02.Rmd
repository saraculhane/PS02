---
title: 'STAT/MATH 495: Problem Set 02'
author: "Sara Culhane, Meredith Manley, Brenna Sullivan"
date: '2017-09-19'
output:
  html_document:
    collapsed: no
    smooth_scroll: no
    toc: yes
    toc_depth: 2
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '2'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width=8, fig.height=4.5)

# Load packages
library(tidyverse)
library(dplyr)
library(mosaic)
library(broom)
require(splines)
library(gridExtra)
# Note the relative file path, and not absolute file path:
# http://www.coffeecup.com/help/articles/absolute-vs-relative-pathslinks/
train <- read_csv("train.csv")
test <- read_csv("test.csv")
train$USD <- (train$price_doc)*0.017 # convert to USD
```


# Exploratory Data Analysis

```{r, include=FALSE}
glimpse(train)
a <- ggplot(train, aes(price_doc))
a + geom_density()
```

Looking at the distribution of housing prices, they were right skewed with a median of 6,274,411 Russian Rubles (determine they were Rubles based on currency conversions).

To choose a predictor we looked at several ggplots and determined that full_sq would be a resonable example though we did not do an exhaustive search.  Since the value of real estate is generally determined by size, we thought that this would be a fair starting place to test out splines and not to worry to much about predictor selection.

We also removed all observations over 2000 square feet but ulimately this only improved our Kaggle score slightly.  However, it greatly improved the correlation between the response and predictor.

```{r}
train2 <- train[train$full_sq < 2000,] #observations of over 2000 removed from the data
ggplot(train2, aes(x=full_sq, y=price_doc)) + geom_point()
cor(train2$full_sq,train2$price_doc)
cor(train$full_sq,train$price_doc)
```

## Spline Visualization

```{r}
values <- data_frame(
  x = train2$full_sq,
  y = train2$price_doc
)




values %>% 
  ggplot(aes(x=x)) +
  geom_point(aes(y=y))
p1 <- smooth.spline(values$x, values$y, df=10) %>%
  broom::augment() %>% 
  ggplot(aes(x=x)) +
  geom_point(aes(y=y)) +
  geom_line(aes(y=.fitted), col="blue", size=1)+
  xlab("Full Square Footage")+
  ylab("Price in Russian Rubles")+
  ggtitle("Fitted Spline",subtitle= "df=10")
p2 <- smooth.spline(values$x, values$y, df=20) %>%
  broom::augment() %>% 
  ggplot(aes(x=x)) +
  geom_point(aes(y=y)) +
  geom_line(aes(y=.fitted), col="blue", size=1)+
  xlab("Full Square Footage")+
  ylab("Price in Russian Rubles")+
  ggtitle("Fitted Spline",subtitle= "df=20")
p3 <- smooth.spline(values$x, values$y, df=30) %>%
  broom::augment() %>% 
  ggplot(aes(x=x)) +
  geom_point(aes(y=y)) +
  geom_line(aes(y=.fitted), col="blue", size=1)+
  xlab("Full Square Footage")+
  ylab("Price in Russian Rubles")+
  ggtitle("Fitted Spline",subtitle= "df=30")
grid.arrange(p1, p2, p3,ncol=3)
```

# Model Fit

Examining the outputs of these models based on 10,20 and 30 df, we found that 20 had rougly the same adjusted $R^2$ as 30 but more significant knots (knots 24 through 27 wre not significant at at 0.05 alpha for df=30).  We also know that higher degrees of freedom tend to overfit, thus since the difference in adjusted $R^2$ seemed negligible, $df=20$ seems to be the better choice.

```{r}
fit<-lm(y ~ bs(x,df=20),data = values )
summary(fit)

fit2<-lm(y ~ bs(x,df=10),data = values )
fit3 <-lm(y ~ bs(x,df=30),data = values )
summary(fit2)
summary(fit3)
f <- makeFun(fit)
test$price_doc <- f(test$full_sq)
sub  <- test[,c(1,length(test))]

```


# Create Submission File

```{r}
write_csv(sub, "submission.csv")
```


Our Kaggle score with our model was 0.4099.